{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "615b25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as utils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ec01919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model runs on GPU if it's available\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Save the dataset at this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "INTERMEDIATE_IMAGES = os.path.join(DATA_DIR_PATH, 'intermediate_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5d832",
   "metadata": {},
   "source": [
    "# Understanding the data\n",
    "I'll be using the MNIST dataset to train the GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4df4850",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the images are: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# In the MNIST dataset in PyTorch, the pixel values are automatically normalized to the range [0, 1]\n",
    "# The images are first converted to PyTorch Tensors\n",
    "# The data is then re-normalized to be in the range [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# The MNIST dataset is a collection of 70,000 handwritten digits (0-9) with 28x28 pixel resolution images\n",
    "# The above transform will be applied to every image in the dataset\n",
    "mnist_dataset = datasets.MNIST(root=DATA_DIR_PATH, train=True, download=True, transform=transform)\n",
    "\n",
    "# Divides the data into batches of size 'batch_size'\n",
    "mnist_data_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Consider the first batch of images\n",
    "images, labels = next(iter(mnist_data_loader)) \n",
    "\n",
    "# What's the shape of the images?\n",
    "print(f'The shape of the images are: {images.shape[1:]}')\n",
    "\n",
    "# Visualize the images\n",
    "no_of_images = 16\n",
    "image_subset = images[:no_of_images] # We take only a subset of images to display\n",
    "\n",
    "grid = utils.make_grid(image_subset, nrow=int(np.sqrt(no_of_images)))\n",
    "\n",
    "plt.title('Images from MNIST dataset')\n",
    "plt.imshow(grid.permute(1, 2, 0)) # Matplotlib takes in images with dimensions of form HWC\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498fac0",
   "metadata": {},
   "source": [
    "## Understanding the Components of GANs\n",
    "GANs have two components:\n",
    "1. <b>Generator Network:</b> The generator network takes a random noise vector as input and generates a new image.\n",
    "2. <b>Discriminator Network:</b> The discriminator network takes an image as input and outputs a scalar value indicating whether the image is real or fake.\n",
    "\n",
    "---\n",
    "\n",
    "### How do the two networks work together?\n",
    "> According to Goodfellow et al. (2014), \"The training procedure for Generator model is to maximize the probability of Discriminator making a mistake. This framework corresponds to a <b>minimax two-player game.<b>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7fe21",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "+ `LeakyReLU` is used as a non-linear activation here.<br>\n",
    "+ `Batch Normalization` is used after each linear layer\n",
    "+ The last layer of the generator network is a `Tanh` function.\n",
    "\n",
    "Learnt some useful insights from https://github.com/soumith/ganhacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fde7e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, image_dim):\n",
    "        super().__init__()\n",
    "        num_neurons_per_layer = [256, 512, 1024]\n",
    "        self.noise_dim = noise_dim\n",
    "        self.image_dim = image_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.noise_dim, num_neurons_per_layer[0]),\n",
    "            nn.BatchNorm1d(num_neurons_per_layer[0]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "           \n",
    "            nn.Linear(num_neurons_per_layer[0], num_neurons_per_layer[1]),\n",
    "            nn.BatchNorm1d(num_neurons_per_layer[1]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(num_neurons_per_layer[1], num_neurons_per_layer[2]),\n",
    "            nn.BatchNorm1d(num_neurons_per_layer[2]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(num_neurons_per_layer[2], self.image_dim),\n",
    "            nn.Tanh() # Normalizes the data into [-1, 1] range                       \n",
    "        )\n",
    "        \n",
    "    def forward(self, noise):\n",
    "        image_batch_flattened = self.net(noise) # flattened image of dimension (N, HXW) N -> batch size\n",
    "        # un-flattening image into dimensions (N, 1, 28, 28) for MNIST\n",
    "        return image_batch_flattened.view(image_batch_flattened.shape[0], 1, 28, 28) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6c980",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "656bbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_dim):\n",
    "        super().__init__()\n",
    "        num_neurons_per_layer = [512, 256, 1]\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(image_dim, num_neurons_per_layer[0]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "           \n",
    "            nn.Linear(num_neurons_per_layer[0], num_neurons_per_layer[1]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(num_neurons_per_layer[1], num_neurons_per_layer[2]),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        \n",
    "    def forward(self, image):\n",
    "        image_batch_flattened = image.view(image.shape[0], -1) # flatten the image to (N, HxW) N -> batch size\n",
    "        return self.net(image_batch_flattened)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef04ebe",
   "metadata": {},
   "source": [
    "## Understanding the Training Process\n",
    "\n",
    "I have made the following changes to the original paper:\n",
    "1. The original paper used <b>Momentum</b> but I've used <b>Adam optimizer</b> \n",
    "2. <b>Hyperparameter</b> values are not the same as experiments in the original paper<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Notations used\n",
    "D - Discriminator Network<br>\n",
    "G - Generator network<br>\n",
    "z - random noise<br>\n",
    "x - Image from the data<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function Intuition\n",
    "<b>Discriminator</b> wants to maximize `log(D(x)) + log(1 - D(G(z)))`<br>\n",
    "<b>Generator</b> wants to minimize the above equation but `X = log(1 - D(G(z)))` part can be considered for minimizing<br>\n",
    "With loss expression X, we see vanishing gradients when D performs very well which is why we maximize the equation `log(D(G(z))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12381680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 3e-4 # A good learning rate for Adam optimizer\n",
    "num_epochs = 10\n",
    "noise_dim = 28 * 28 * 1\n",
    "image_dim = 28 * 28 * 1 # The dimensions of images in MNIST dataset\n",
    "img_cnt = 0\n",
    "int_img_grid_size = 16\n",
    "\n",
    "checkpoint_freq = 2\n",
    "console_log_freq = 50\n",
    "intermediate_image_log_freq = 50\n",
    "\n",
    "generator_loss_values = []\n",
    "discriminator_loss_values = []\n",
    "\n",
    "# GAN uses Binary Cross Entropy for it's loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "discriminator_net = Discriminator(noise_dim).train().to(device)\n",
    "generator_net = Generator(noise_dim, image_dim).train().to(device)\n",
    "\n",
    "# We use Adam optimizer for both the neural networks \n",
    "discriminator_opt = Adam(discriminator_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "generator_opt = Adam(generator_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Used later to customize the BCELoss function expression according to needs\n",
    "ones_tensor = torch.ones((batch_size, 1), device=device)\n",
    "zeros_tensor = torch.zeros((batch_size, 1), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f051182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN training: time elapsed = 0.13 [s] | epoch=1 | batch= [1/468]\n",
      "Loss D: 1.3752, Loss G: 0.7345\n",
      "GAN training: time elapsed = 3.74 [s] | epoch=1 | batch= [51/468]\n",
      "Loss D: 1.1199, Loss G: 1.7374\n",
      "GAN training: time elapsed = 7.35 [s] | epoch=1 | batch= [101/468]\n",
      "Loss D: 1.2940, Loss G: 0.7388\n",
      "GAN training: time elapsed = 10.54 [s] | epoch=1 | batch= [151/468]\n",
      "Loss D: 1.4556, Loss G: 1.2991\n",
      "GAN training: time elapsed = 13.79 [s] | epoch=1 | batch= [201/468]\n",
      "Loss D: 1.1780, Loss G: 0.7239\n",
      "GAN training: time elapsed = 17.00 [s] | epoch=1 | batch= [251/468]\n",
      "Loss D: 1.2625, Loss G: 0.9817\n",
      "GAN training: time elapsed = 20.24 [s] | epoch=1 | batch= [301/468]\n",
      "Loss D: 1.3159, Loss G: 0.6466\n",
      "GAN training: time elapsed = 23.72 [s] | epoch=1 | batch= [351/468]\n",
      "Loss D: 1.3180, Loss G: 0.8588\n",
      "GAN training: time elapsed = 27.05 [s] | epoch=1 | batch= [401/468]\n",
      "Loss D: 1.1322, Loss G: 0.9094\n",
      "GAN training: time elapsed = 30.29 [s] | epoch=1 | batch= [451/468]\n",
      "Loss D: 1.4850, Loss G: 0.5870\n",
      "GAN training: time elapsed = 31.47 [s] | epoch=2 | batch= [1/468]\n",
      "Loss D: 1.2591, Loss G: 0.9464\n",
      "GAN training: time elapsed = 34.77 [s] | epoch=2 | batch= [51/468]\n",
      "Loss D: 1.2189, Loss G: 0.8921\n",
      "GAN training: time elapsed = 38.08 [s] | epoch=2 | batch= [101/468]\n",
      "Loss D: 1.1730, Loss G: 0.7206\n",
      "GAN training: time elapsed = 41.31 [s] | epoch=2 | batch= [151/468]\n",
      "Loss D: 1.2444, Loss G: 1.1654\n",
      "GAN training: time elapsed = 44.49 [s] | epoch=2 | batch= [201/468]\n",
      "Loss D: 1.4864, Loss G: 0.5551\n",
      "GAN training: time elapsed = 47.64 [s] | epoch=2 | batch= [251/468]\n",
      "Loss D: 1.1806, Loss G: 0.8882\n",
      "GAN training: time elapsed = 50.83 [s] | epoch=2 | batch= [301/468]\n",
      "Loss D: 1.1955, Loss G: 0.9618\n",
      "GAN training: time elapsed = 54.01 [s] | epoch=2 | batch= [351/468]\n",
      "Loss D: 1.2396, Loss G: 0.8149\n",
      "GAN training: time elapsed = 57.18 [s] | epoch=2 | batch= [401/468]\n",
      "Loss D: 1.0787, Loss G: 1.0303\n",
      "GAN training: time elapsed = 60.33 [s] | epoch=2 | batch= [451/468]\n",
      "Loss D: 1.1672, Loss G: 0.8005\n",
      "GAN training: time elapsed = 61.50 [s] | epoch=3 | batch= [1/468]\n",
      "Loss D: 1.2294, Loss G: 0.9143\n",
      "GAN training: time elapsed = 64.69 [s] | epoch=3 | batch= [51/468]\n",
      "Loss D: 1.1837, Loss G: 1.3373\n",
      "GAN training: time elapsed = 67.86 [s] | epoch=3 | batch= [101/468]\n",
      "Loss D: 1.1902, Loss G: 1.2301\n",
      "GAN training: time elapsed = 71.03 [s] | epoch=3 | batch= [151/468]\n",
      "Loss D: 1.1273, Loss G: 1.3256\n",
      "GAN training: time elapsed = 74.20 [s] | epoch=3 | batch= [201/468]\n",
      "Loss D: 1.1159, Loss G: 1.0135\n",
      "GAN training: time elapsed = 77.38 [s] | epoch=3 | batch= [251/468]\n",
      "Loss D: 1.1540, Loss G: 0.8050\n",
      "GAN training: time elapsed = 80.56 [s] | epoch=3 | batch= [301/468]\n",
      "Loss D: 1.2063, Loss G: 1.0476\n",
      "GAN training: time elapsed = 83.76 [s] | epoch=3 | batch= [351/468]\n",
      "Loss D: 1.3214, Loss G: 1.0627\n",
      "GAN training: time elapsed = 87.17 [s] | epoch=3 | batch= [401/468]\n",
      "Loss D: 1.1739, Loss G: 0.8981\n",
      "GAN training: time elapsed = 90.36 [s] | epoch=3 | batch= [451/468]\n",
      "Loss D: 1.4183, Loss G: 2.7370\n",
      "GAN training: time elapsed = 91.60 [s] | epoch=4 | batch= [1/468]\n",
      "Loss D: 1.3591, Loss G: 0.4768\n",
      "GAN training: time elapsed = 95.08 [s] | epoch=4 | batch= [51/468]\n",
      "Loss D: 1.0828, Loss G: 1.2587\n",
      "GAN training: time elapsed = 98.50 [s] | epoch=4 | batch= [101/468]\n",
      "Loss D: 1.1507, Loss G: 1.1127\n",
      "GAN training: time elapsed = 101.82 [s] | epoch=4 | batch= [151/468]\n",
      "Loss D: 1.0745, Loss G: 0.9957\n",
      "GAN training: time elapsed = 105.24 [s] | epoch=4 | batch= [201/468]\n",
      "Loss D: 1.1591, Loss G: 1.0124\n",
      "GAN training: time elapsed = 108.53 [s] | epoch=4 | batch= [251/468]\n",
      "Loss D: 1.0155, Loss G: 1.1616\n",
      "GAN training: time elapsed = 111.81 [s] | epoch=4 | batch= [301/468]\n",
      "Loss D: 1.0865, Loss G: 1.0533\n",
      "GAN training: time elapsed = 115.02 [s] | epoch=4 | batch= [351/468]\n",
      "Loss D: 1.1317, Loss G: 2.0443\n",
      "GAN training: time elapsed = 118.29 [s] | epoch=4 | batch= [401/468]\n",
      "Loss D: 1.1653, Loss G: 1.1320\n",
      "GAN training: time elapsed = 121.49 [s] | epoch=4 | batch= [451/468]\n",
      "Loss D: 1.1130, Loss G: 0.9753\n",
      "GAN training: time elapsed = 122.67 [s] | epoch=5 | batch= [1/468]\n",
      "Loss D: 1.1783, Loss G: 1.0279\n",
      "GAN training: time elapsed = 125.95 [s] | epoch=5 | batch= [51/468]\n",
      "Loss D: 1.1105, Loss G: 0.8922\n",
      "GAN training: time elapsed = 129.35 [s] | epoch=5 | batch= [101/468]\n",
      "Loss D: 1.1974, Loss G: 1.0841\n",
      "GAN training: time elapsed = 132.70 [s] | epoch=5 | batch= [151/468]\n",
      "Loss D: 1.0818, Loss G: 0.9142\n",
      "GAN training: time elapsed = 135.92 [s] | epoch=5 | batch= [201/468]\n",
      "Loss D: 1.0401, Loss G: 1.0193\n",
      "GAN training: time elapsed = 139.20 [s] | epoch=5 | batch= [251/468]\n",
      "Loss D: 1.2491, Loss G: 1.4420\n",
      "GAN training: time elapsed = 142.38 [s] | epoch=5 | batch= [301/468]\n",
      "Loss D: 1.3493, Loss G: 2.2352\n",
      "GAN training: time elapsed = 145.55 [s] | epoch=5 | batch= [351/468]\n",
      "Loss D: 1.1580, Loss G: 1.2565\n",
      "GAN training: time elapsed = 148.78 [s] | epoch=5 | batch= [401/468]\n",
      "Loss D: 1.1514, Loss G: 1.1462\n",
      "GAN training: time elapsed = 152.16 [s] | epoch=5 | batch= [451/468]\n",
      "Loss D: 1.0705, Loss G: 1.1129\n",
      "GAN training: time elapsed = 153.37 [s] | epoch=6 | batch= [1/468]\n",
      "Loss D: 1.0634, Loss G: 0.9054\n",
      "GAN training: time elapsed = 156.57 [s] | epoch=6 | batch= [51/468]\n",
      "Loss D: 1.1532, Loss G: 1.0057\n",
      "GAN training: time elapsed = 159.75 [s] | epoch=6 | batch= [101/468]\n",
      "Loss D: 1.1663, Loss G: 1.0603\n",
      "GAN training: time elapsed = 162.90 [s] | epoch=6 | batch= [151/468]\n",
      "Loss D: 1.1827, Loss G: 1.0065\n",
      "GAN training: time elapsed = 166.06 [s] | epoch=6 | batch= [201/468]\n",
      "Loss D: 1.3155, Loss G: 0.5743\n",
      "GAN training: time elapsed = 169.21 [s] | epoch=6 | batch= [251/468]\n",
      "Loss D: 1.0983, Loss G: 1.1790\n",
      "GAN training: time elapsed = 172.37 [s] | epoch=6 | batch= [301/468]\n",
      "Loss D: 1.2603, Loss G: 1.6377\n",
      "GAN training: time elapsed = 175.53 [s] | epoch=6 | batch= [351/468]\n",
      "Loss D: 1.2048, Loss G: 1.2162\n",
      "GAN training: time elapsed = 178.72 [s] | epoch=6 | batch= [401/468]\n",
      "Loss D: 1.2687, Loss G: 0.7876\n",
      "GAN training: time elapsed = 181.88 [s] | epoch=6 | batch= [451/468]\n",
      "Loss D: 1.1798, Loss G: 0.9034\n",
      "GAN training: time elapsed = 183.04 [s] | epoch=7 | batch= [1/468]\n",
      "Loss D: 1.1107, Loss G: 0.8975\n",
      "GAN training: time elapsed = 186.22 [s] | epoch=7 | batch= [51/468]\n",
      "Loss D: 1.2228, Loss G: 0.7849\n",
      "GAN training: time elapsed = 189.60 [s] | epoch=7 | batch= [101/468]\n",
      "Loss D: 1.5991, Loss G: 0.5393\n",
      "GAN training: time elapsed = 192.95 [s] | epoch=7 | batch= [151/468]\n",
      "Loss D: 1.2354, Loss G: 0.8277\n",
      "GAN training: time elapsed = 196.22 [s] | epoch=7 | batch= [201/468]\n",
      "Loss D: 1.1479, Loss G: 0.9992\n",
      "GAN training: time elapsed = 199.39 [s] | epoch=7 | batch= [251/468]\n",
      "Loss D: 1.2408, Loss G: 0.7065\n",
      "GAN training: time elapsed = 202.68 [s] | epoch=7 | batch= [301/468]\n",
      "Loss D: 1.1984, Loss G: 0.8462\n",
      "GAN training: time elapsed = 205.98 [s] | epoch=7 | batch= [351/468]\n",
      "Loss D: 1.2655, Loss G: 0.7501\n",
      "GAN training: time elapsed = 209.33 [s] | epoch=7 | batch= [401/468]\n",
      "Loss D: 1.2331, Loss G: 1.3214\n",
      "GAN training: time elapsed = 212.67 [s] | epoch=7 | batch= [451/468]\n",
      "Loss D: 1.3919, Loss G: 1.4396\n",
      "GAN training: time elapsed = 214.04 [s] | epoch=8 | batch= [1/468]\n",
      "Loss D: 1.1852, Loss G: 1.4264\n",
      "GAN training: time elapsed = 217.34 [s] | epoch=8 | batch= [51/468]\n",
      "Loss D: 1.1662, Loss G: 0.9785\n",
      "GAN training: time elapsed = 220.64 [s] | epoch=8 | batch= [101/468]\n",
      "Loss D: 1.2502, Loss G: 1.5382\n",
      "GAN training: time elapsed = 223.88 [s] | epoch=8 | batch= [151/468]\n",
      "Loss D: 1.2114, Loss G: 0.8946\n",
      "GAN training: time elapsed = 227.08 [s] | epoch=8 | batch= [201/468]\n",
      "Loss D: 1.1725, Loss G: 1.1620\n",
      "GAN training: time elapsed = 230.26 [s] | epoch=8 | batch= [251/468]\n",
      "Loss D: 1.2501, Loss G: 0.9591\n",
      "GAN training: time elapsed = 233.46 [s] | epoch=8 | batch= [301/468]\n",
      "Loss D: 1.2348, Loss G: 0.7753\n",
      "GAN training: time elapsed = 236.67 [s] | epoch=8 | batch= [351/468]\n",
      "Loss D: 1.3062, Loss G: 0.5645\n",
      "GAN training: time elapsed = 239.84 [s] | epoch=8 | batch= [401/468]\n",
      "Loss D: 1.1571, Loss G: 1.2137\n",
      "GAN training: time elapsed = 243.20 [s] | epoch=8 | batch= [451/468]\n",
      "Loss D: 1.2917, Loss G: 0.7833\n",
      "GAN training: time elapsed = 244.39 [s] | epoch=9 | batch= [1/468]\n",
      "Loss D: 1.3308, Loss G: 0.6054\n",
      "GAN training: time elapsed = 247.61 [s] | epoch=9 | batch= [51/468]\n",
      "Loss D: 1.2967, Loss G: 0.7896\n",
      "GAN training: time elapsed = 250.85 [s] | epoch=9 | batch= [101/468]\n",
      "Loss D: 1.3026, Loss G: 0.5623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN training: time elapsed = 254.24 [s] | epoch=9 | batch= [151/468]\n",
      "Loss D: 1.1602, Loss G: 0.9813\n",
      "GAN training: time elapsed = 257.54 [s] | epoch=9 | batch= [201/468]\n",
      "Loss D: 1.1925, Loss G: 1.0226\n",
      "GAN training: time elapsed = 260.72 [s] | epoch=9 | batch= [251/468]\n",
      "Loss D: 1.1985, Loss G: 0.7715\n",
      "GAN training: time elapsed = 264.06 [s] | epoch=9 | batch= [301/468]\n",
      "Loss D: 1.3171, Loss G: 0.6959\n",
      "GAN training: time elapsed = 267.25 [s] | epoch=9 | batch= [351/468]\n",
      "Loss D: 1.1751, Loss G: 0.9240\n",
      "GAN training: time elapsed = 270.47 [s] | epoch=9 | batch= [401/468]\n",
      "Loss D: 1.1696, Loss G: 0.9197\n",
      "GAN training: time elapsed = 273.59 [s] | epoch=9 | batch= [451/468]\n",
      "Loss D: 1.3627, Loss G: 1.4276\n",
      "GAN training: time elapsed = 274.75 [s] | epoch=10 | batch= [1/468]\n",
      "Loss D: 1.2567, Loss G: 0.6279\n",
      "GAN training: time elapsed = 277.94 [s] | epoch=10 | batch= [51/468]\n",
      "Loss D: 1.1966, Loss G: 0.9468\n",
      "GAN training: time elapsed = 281.11 [s] | epoch=10 | batch= [101/468]\n",
      "Loss D: 1.1996, Loss G: 1.0336\n",
      "GAN training: time elapsed = 284.43 [s] | epoch=10 | batch= [151/468]\n",
      "Loss D: 1.1594, Loss G: 1.1603\n",
      "GAN training: time elapsed = 287.71 [s] | epoch=10 | batch= [201/468]\n",
      "Loss D: 1.2276, Loss G: 0.7531\n",
      "GAN training: time elapsed = 290.93 [s] | epoch=10 | batch= [251/468]\n",
      "Loss D: 1.2562, Loss G: 0.6673\n",
      "GAN training: time elapsed = 294.12 [s] | epoch=10 | batch= [301/468]\n",
      "Loss D: 1.1401, Loss G: 0.8851\n",
      "GAN training: time elapsed = 297.47 [s] | epoch=10 | batch= [351/468]\n",
      "Loss D: 1.1672, Loss G: 0.9354\n",
      "GAN training: time elapsed = 300.61 [s] | epoch=10 | batch= [401/468]\n",
      "Loss D: 1.2134, Loss G: 0.8445\n",
      "GAN training: time elapsed = 303.80 [s] | epoch=10 | batch= [451/468]\n",
      "Loss D: 1.3670, Loss G: 0.7204\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(mnist_data_loader):\n",
    "        \n",
    "        #-----------------------------------         \n",
    "        # Training the discriminator network\n",
    "        #-----------------------------------\n",
    "        \n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # PyTorch accumulates gradients over the epochs\n",
    "        # Set the gradients at each step to zero\n",
    "        discriminator_opt.zero_grad()\n",
    "        \n",
    "        # To configure the BCELoss to -log(D(x)), we pass in a tensor of ones \n",
    "        real_discriminator_loss = adversarial_loss(discriminator_net(real_images), ones_tensor) \n",
    "        \n",
    "        \n",
    "        # create random input noise for the generator\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)         \n",
    "        fake_images = generator_net(noise)                                        \n",
    "\n",
    "        # To configure the BCELoss to -log(1 - D(G(z))), we pass in a tensor of zeros\n",
    "        fake_images_predictions = discriminator_net(fake_images)\n",
    "        fake_discriminator_loss = adversarial_loss(fake_images_predictions, zeros_tensor)\n",
    "        \n",
    "        discriminator_loss = real_discriminator_loss + fake_discriminator_loss\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_opt.step()\n",
    "        \n",
    "        #-----------------------------------         \n",
    "        # Training the Generator network\n",
    "        #-----------------------------------\n",
    "        \n",
    "        generator_opt.zero_grad()\n",
    "        generator_training_noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake_images_generated = generator_net(generator_training_noise)\n",
    "        \n",
    "        generated_image_predictions = discriminator_net(fake_images_generated)\n",
    "        \n",
    "        # To configure the BCELoss to -log(D(G(z))), we pass in a tensor of ones\n",
    "        generator_loss = adversarial_loss(generated_image_predictions, ones_tensor)\n",
    "        generator_loss.backward()\n",
    "        generator_opt.step()\n",
    "        \n",
    "        generator_loss_values.append(generator_loss.item())\n",
    "        discriminator_loss_values.append(discriminator_loss.item())\n",
    "        \n",
    "        # Save intermediate generator images \n",
    "        if batch_idx % intermediate_image_log_freq == 0:\n",
    "            with torch.no_grad():\n",
    "                log_generated_images = generator_net(noise)\n",
    "                log_generated_images_resized = nn.Upsample(scale_factor=2.5, mode='nearest')(log_generated_images)\n",
    "                out_path = os.path.join(INTERMEDIATE_IMAGES, f'{str(img_cnt).zfill(6)}.jpg')\n",
    "                save_image(log_generated_images_resized, out_path, nrow=int(np.sqrt(int_img_grid_size)), normalize=True)\n",
    "                img_cnt += 1\n",
    "        \n",
    "        if batch_idx % console_log_freq == 0:\n",
    "            prefix = 'GAN training: time elapsed'\n",
    "            print(f'{prefix} = {(time.time() - ts):.2f} [s] | epoch={epoch + 1} | batch= [{batch_idx + 1}/{len(mnist_data_loader)}]')\n",
    "            print(f\"Loss D: {discriminator_loss:.4f}, Loss G: {generator_loss:.4f}\")\n",
    "        \n",
    "        # Save generator checkpoint\n",
    "        if (epoch + 1) % checkpoint_freq == 0 and (batch_idx + 1) == 0:\n",
    "            ckpt_model_name = f\"vanilla_ckpt_epoch_{epoch + 1}_batch_{batch_idx + 1}.pth\"\n",
    "            torch.save(generator_net.state_dict(), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))\n",
    "    \n",
    "torch.save(generator_net.state_dict(), os.path.join(BINARIES_PATH, \"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f970b",
   "metadata": {},
   "source": [
    "## Generating new images\n",
    "https://github.com/gordicaleksa/pytorch-GANs is used as a reference for the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "527e52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_generated_img(generated_img_tensor):\n",
    "    assert isinstance(generated_img_tensor, torch.Tensor), f'Expected PyTorch tensor but got {type(generated_img_tensor)}.'\n",
    "\n",
    "    # Move the tensor from GPU to CPU, convert to numpy array, extract 0th batch, move the image channel\n",
    "    # from 0th to 2nd position (CHW -> HWC)\n",
    "    generated_img = np.moveaxis(generated_img_tensor.to('cpu').numpy()[0], 0, 2)\n",
    "\n",
    "    # Since MNIST images are grayscale (1-channel only) repeat 3 times to get RGB image\n",
    "    generated_img = np.repeat(generated_img,  3, axis=2)\n",
    "\n",
    "    # Imagery is in the range [-1, 1] (generator has tanh as the output activation) move it into [0, 1] range\n",
    "    generated_img -= np.min(generated_img)\n",
    "    generated_img /= np.max(generated_img)\n",
    "\n",
    "    return generated_img\n",
    "\n",
    "def generate_from_random_noise(generator):\n",
    "    with torch.no_grad():  # Tells PyTorch not to compute gradients which would have huge memory footprint\n",
    "        \n",
    "        # Generate a single random (latent) vector\n",
    "        latent_vector = torch.randn(1, noise_dim).to(device)\n",
    "        \n",
    "        # Post process generator output (as it's in the [-1, 1] range)\n",
    "        generated_img = postprocess_generated_img(generator(latent_vector))\n",
    "\n",
    "    return generated_img\n",
    "\n",
    "def save_and_maybe_display_image(dump_dir, dump_img, out_res=(256, 256), should_display=False):\n",
    "    assert isinstance(dump_img, np.ndarray), f'Expected numpy array got {type(dump_img)}.'\n",
    "\n",
    "    # step1: get next valid image name\n",
    "    dump_img_name = \"generated_image.jpg\"\n",
    "\n",
    "    # step2: convert to uint8 format <- OpenCV expects it otherwise your image will be completely black. Don't ask...\n",
    "    if dump_img.dtype != np.uint8:\n",
    "        dump_img = (dump_img*255).astype(np.uint8)\n",
    "\n",
    "    # step3: write image to the file system (::-1 because opencv expects BGR (and not RGB) format...)\n",
    "    cv.imwrite(os.path.join(dump_dir, dump_img_name), cv.resize(dump_img[:, :, ::-1], out_res, interpolation=cv.INTER_NEAREST))  \n",
    "\n",
    "    # step4: maybe display part of the function\n",
    "    if should_display:\n",
    "        plt.imshow(dump_img)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7fa980e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = os.path.join(BINARIES_PATH, 'model.pth')\n",
    "\n",
    "model_state = torch.load(model_path)\n",
    "\n",
    "generator = Generator(28 * 28, 28 * 28).to(device)\n",
    "\n",
    "generator.load_state_dict(generator_net.state_dict())\n",
    "generator.eval()\n",
    "\n",
    "generated_imgs_path = os.path.join(DATA_DIR_PATH, 'generated_imagery')  # this is where we'll dump images\n",
    "os.makedirs(generated_imgs_path, exist_ok=True)\n",
    "\n",
    "generated_img = generate_from_random_noise(generator)\n",
    "save_and_maybe_display_image(generated_imgs_path, generated_img, should_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ca88c",
   "metadata": {},
   "source": [
    "## PyTorch GANs :computer: :art:\n",
    "This repo contains PyTorch implementation of Vanilla GAN architecture. <br/>\n",
    "\n",
    "## Table of Contents\n",
    "  * [Understanding GANs](#Understanding-GANs)\n",
    "  * [Vanilla GAN](#vanilla-gan)\n",
    "    \n",
    "---\n",
    "\n",
    "## Understanding GANs\n",
    "\n",
    "GAN stands for Generative Adversarial Networks, which is a type of deep learning model that consists of two networks: a generator and a discriminator. The generator network learns to generate realistic-looking fake data (e.g. images, audio, text) from random noise, while the discriminator network learns to distinguish the fake data from the real data. The two networks are trained simultaneously in an adversarial manner, where the generator tries to fool the discriminator by generating more realistic fake data, while the discriminator tries to correctly identify the real and fake data.\n",
    "\n",
    "The original paper introducing GANs is titled [Generative Adversarial Networks](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) and was authored by Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. It was published in 2014 at the Conference on Neural Information Processing Systems (NIPS).\n",
    "\n",
    "GANs have two components:\n",
    "1. <b>Generator Network:</b> The generator network takes a random noise vector as input and generates a new image.\n",
    "2. <b>Discriminator Network:</b> The discriminator network takes an image as input and outputs a scalar value indicating whether the image is real or fake.\n",
    "\n",
    "---\n",
    "\n",
    "## Vanilla GAN\n",
    "\n",
    "Vanilla GAN is my implementation of the original GAN paper with certain modifications mostly in the model architecture,\n",
    "like the usage of LeakyReLU and 1D batch normalization.\n",
    "\n",
    "### Examples\n",
    "    \n",
    "GAN was trained on data from MNIST dataset. Here is how the digits from the dataset look like:\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"data/generated_imagery/generated_image0.jpg\" style=\"width: 30%; display: inline-block;\">\n",
    "    <img src=\"data/generated_imagery/generated_image.jpg\" style=\"width: 30%; display: inline-block;\">\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "I've used the following repositories as reference for implementing my version:\n",
    "* [pytorch-GANs](https://github.com/gordicaleksa/pytorch-GANs) (PyTorch)\n",
    "* [PyTorch-GAN](https://github.com/ahmadchalhoub/research_implementations) (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa0978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
